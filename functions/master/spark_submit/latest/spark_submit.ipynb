{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submiting spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mlrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "import nuclio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the MLRun environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%nuclio: setting kind to 'job'\n",
      "%nuclio: setting spec.image to 'mlrun/mlrun'\n"
     ]
    }
   ],
   "source": [
    "%nuclio config kind = \"job\"\n",
    "%nuclio config spec.image = \"mlrun/mlrun\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: start-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import get_or_create_ctx\n",
    "from kubernetes import config, client\n",
    "from kubernetes.stream import stream\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K8SClient(object):\n",
    "\n",
    "    def __init__(self, logger, namespace='default-tenant', config_file=None):\n",
    "        self.namespace = namespace\n",
    "        self.logger = logger\n",
    "        self._init_k8s_config(config_file)\n",
    "        self.v1api = client.CoreV1Api()\n",
    "\n",
    "    def _init_k8s_config(self, config_file):\n",
    "        try:\n",
    "            config.load_incluster_config()\n",
    "            self.logger.info('using in-cluster config.')\n",
    "        except Exception:\n",
    "            try:\n",
    "                config.load_kube_config(config_file)\n",
    "                self.logger.info('using local kubernetes config.')\n",
    "            except Exception:\n",
    "                raise RuntimeError(\n",
    "                    'cannot find local kubernetes config file,'\n",
    "                    ' place it in ~/.kube/config or specify it in '\n",
    "                    'KUBECONFIG env var')\n",
    "\n",
    "    def get_shell_pod_name(self, pod_name='shell'):\n",
    "        shell_pod = self.v1api.list_namespaced_pod(namespace=self.namespace)\n",
    "        for i in shell_pod.items:\n",
    "            if pod_name in i.metadata.name:\n",
    "                self.logger.info(\"%s\\t%s\\t%s\" % (i.status.pod_ip, i.metadata.namespace, i.metadata.name))\n",
    "                shell_name = i.metadata.name\n",
    "                break\n",
    "        return shell_name\n",
    "\n",
    "    def exec_shell_cmd(self, cmd, shell_pod_name = 'shell'):\n",
    "        shell_name = self.get_shell_pod_name(shell_pod_name)\n",
    "        # Calling exec and waiting for response\n",
    "        exec_command = [\n",
    "            '/bin/bash',\n",
    "            '-c',\n",
    "            cmd]\n",
    "        resp = stream(self.v1api.connect_get_namespaced_pod_exec,\n",
    "                      shell_name,\n",
    "                      self.namespace,\n",
    "                      command=exec_command,\n",
    "                      stderr=True, stdin=False,\n",
    "                      stdout=True, tty=False,_preload_content=False)\n",
    "        \n",
    "        \n",
    "        stderr = []\n",
    "        stdout = []\n",
    "        while resp.is_open():\n",
    "            resp.update(timeout=None)\n",
    "            if resp.peek_stderr():\n",
    "                 stderr.append(resp.read_stderr())\n",
    "            if resp.peek_stdout():\n",
    "                 stdout.append(resp.read_stdout())\n",
    "           \n",
    "        err = resp.read_channel(3)\n",
    "        err = yaml.safe_load(err)\n",
    "        if err['status'] == 'Success':\n",
    "            rc = 0\n",
    "        else:\n",
    "            rc = int(err['details']['causes'][0]['message'])\n",
    "        \n",
    "        stdout_formated=''\n",
    "        for each in stdout:\n",
    "            stdout_formated += each\n",
    "        self.logger.info(\"STDOUT: %s\"% stdout_formated)\n",
    "        self.logger.info(\"RC: %s\"% rc)\n",
    "        \n",
    "        stderr_formated=''\n",
    "        for each in stderr:\n",
    "            stderr_formated += each\n",
    "        if rc != 0:\n",
    "            self.logger.info(\"STDERR: %s\"% stderr_formated)\n",
    "            raise Exception('Execution_error', stderr_formated + stdout_formated)\n",
    "    \n",
    "def nb_or_py(pyspark_command):\n",
    "    if pyspark_command.endswith('.ipynb'):\n",
    "        from mlrun import code_to_function\n",
    "        dir = Path(pyspark_command).parent\n",
    "        file =  Path(pyspark_command).stem\n",
    "        python_file = os.path.join(dir,file +'.py')\n",
    "        code_to_function(kind=\"local\",filename=pyspark_command,code_output=python_file)\n",
    "        return python_file\n",
    "    else:\n",
    "        return pyspark_command     \n",
    "\n",
    "\n",
    "def spark_command_builder(name, class_name, jars, packages, spark_options,pyspark_command='',pyspark_params=''):\n",
    "    cmd = 'spark-submit'\n",
    "    if name:\n",
    "        cmd += ' --name ' + name\n",
    "\n",
    "    if class_name:\n",
    "        cmd += ' --class ' + class_name\n",
    "\n",
    "    if jars:\n",
    "        cmd += ' --jars ' + jars\n",
    "\n",
    "    if packages:\n",
    "        cmd += ' --packages ' + packages\n",
    "\n",
    "    if spark_options:\n",
    "        cmd += ' ' + spark_options\n",
    "\n",
    "    if pyspark_command:\n",
    "        cmd += ' ' + nb_or_py(pyspark_command)\n",
    "            \n",
    "    if pyspark_params:\n",
    "        cmd += ' ' + pyspark_params\n",
    "        \n",
    "    return cmd\n",
    "\n",
    "\n",
    "def spark_submit(context, v3io_access_key, name=None, class_name=None, jars=None, packages=None, spark_options='',pyspark_command='',pyspark_params='',shell_pod_name='shell'):\n",
    "    \"\"\"spark_submit function\n",
    "    \n",
    "    submiting spark via shell\n",
    "    \n",
    "    :param name:        A name of your application.\n",
    "    :param class_name:  Your application's main class (for Java / Scala apps).\n",
    "                        * If relative will add to the {artifact_path}\n",
    "    :param jars:        Comma-separated list of jars to include on the driver\n",
    "                        and executor classpaths.\n",
    "    :param packages:    Comma-separated list of maven coordinates of jars to include\n",
    "                        on the driver and executor classpaths. Will search the local\n",
    "                        maven repo, then maven central and any additional remote\n",
    "                        repositories given by --repositories. The format for the\n",
    "    :param spark_options: spark parametes that are not included as function arguments\n",
    "    :param pyspark_command: A python script or Jupyter notebook to be executed\n",
    "    :param pyspark_params:  parameters to the python script\n",
    "\n",
    "    \"\"\"\n",
    "    cmd = spark_command_builder(name, class_name, jars, packages, spark_options,pyspark_command,pyspark_params)\n",
    "    context.logger.info(\"submiting :\" + cmd)\n",
    "\n",
    "    cli = K8SClient(context.logger)\n",
    "    cli.exec_shell_cmd(cmd,shell_pod_name=shell_pod_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "> This test uses the metrics data, created by the [Generator function](https://github.com/mlrun/demo-network-operations/blob/master/notebooks/generator.ipynb) from MLRun's [Network Operations Demo](https://github.com/mlrun/demo-network-operations)  \n",
    "To test it yourself, please generate this dataset or use any of your available csv/parquet datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mlrun import code_to_function, mount_v3io, NewTask, mlconf, run_local\n",
    "mlconf.dbpath = mlconf.dbpath or 'http://mlrun-api:8080'\n",
    "mlconf.artifact_path = mlconf.artifact_path or f'{os.environ[\"HOME\"]}/artifacts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the execute test task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_task = NewTask(name='spark-submit',\n",
    "                         project='submit-proj',\n",
    "                         params={'pyspark_command':\"/User/tmp/test.ipynb\", \"pyspark_params\": \"12\"},\n",
    "                         handler=spark_submit)\n",
    "\n",
    "## Java Example parameters\n",
    "#params={'spark_options':\"/spark/examples/jars/spark-examples_2.11-2.4.4.jar 10\",'class_name':'org.apache.spark.examples.SparkPi'},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submit_run = run_local(submit_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the code to an MLRun function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = code_to_function('spark-submit', handler='spark_submit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7fd4aa79ca50>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.spec.service_account='mlrun-api'\n",
    "fn.apply(mount_v3io())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2020-11-30 14:43:36,470 [info] function spec saved to path: function.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.runtimes.kubejob.KubejobRuntime at 0x7fd4a39d1b50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn.export('function.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_run = fn.run(execute_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
