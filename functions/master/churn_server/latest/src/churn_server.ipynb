{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Churn Server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cloudpickle import load\n",
    "\n",
    "\n",
    "import mlrun\n",
    "class ChurnModel(mlrun.serving.V2ModelServer):\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        load multiple models in nested folders, churn model only\n",
    "        \"\"\"\n",
    "        clf_model_file, extra_data = self.get_model(\".pkl\")\n",
    "        self.model = load(open(str(clf_model_file), \"rb\"))\n",
    "        if \"cox\" in extra_data.keys():\n",
    "            cox_model_file = extra_data[\"cox\"]\n",
    "            self.cox_model = load(open(str(cox_model_file), \"rb\"))\n",
    "            if \"cox/km\" in extra_data.keys():\n",
    "                km_model_file = extra_data[\"cox/km\"]\n",
    "                self.km_model = load(open(str(km_model_file), \"rb\"))\n",
    "\n",
    "    def predict(self, body):\n",
    "        try:\n",
    "            # we have potentially 3 models to work with:\n",
    "            #if hasattr(self, \"cox_model\") and hasattr(self, \"km_model\"):\n",
    "                # hack for now, just predict using one:\n",
    "            feats = np.asarray(body[\"instances\"], dtype=np.float32).reshape(-1, 23)\n",
    "            result = self.model.predict(feats, validate_features=False)\n",
    "            return result.tolist()\n",
    "            #else:\n",
    "            #    raise Exception(\"models not found\")\n",
    "        except Exception as e:\n",
    "            raise Exception(\"Failed to predict %s\" % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import mlconf\n",
    "import os\n",
    "mlconf.dbpath = mlconf.dbpath or \"http://mlrun-api:8080\"\n",
    "mlconf.artifact_path = mlconf.artifact_path or f\"{os.environ['HOME']}/artifacts\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_URL = f\"https://raw.githubusercontent.com/yjb-ds/testdata/master/demos/churn/churn-tests.csv\"\n",
    "xtest = pd.read_csv(DATA_URL)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deploy\"></a>\n",
    "### Deploy our serving class using as a serverless function\n",
    "in the following section we create a new model serving function which wraps our class , and specify model and other resources.\n",
    "\n",
    "the `models` dict store model names and the assosiated model **dir** URL (the URL can start with `S3://` and other blob store options), the faster way is to use a shared file volume, we use `.apply(mount_v3io())` to attach a v3io (iguazio data fabric) volume to our function. By default v3io will mount the current user home into the `\\User` function path.\n",
    "\n",
    "**verify the model dir does contain a valid `model.bst` file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import import_function\n",
    "from mlrun.platforms.other import auto_mount\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fn = import_function(\"hub://churn_server\")\n",
    "\n",
    "model_dir = os.path.join(mlconf.artifact_path, \"churn\", \"models\")\n",
    "fn.add_model(\"churn_server_v1\", model_path=model_dir)\n",
    "\n",
    "fn.apply(auto_mount())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addr = fn.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **test our model server using HTTP request**\n",
    "We invoke our model serving function using test data, the data vector is specified in the `instances` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFServing protocol event\n",
    "event_data = {\"instances\": xtest.values[:10,:-1].tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "resp = requests.put(addr + \"/churn_server_v1/predict\", json=json.dumps(event_data))\n",
    "\n",
    "tl = resp.text.replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")\n",
    "tl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[back to top](#top)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}